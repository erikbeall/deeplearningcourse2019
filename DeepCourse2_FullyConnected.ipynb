{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: A Simple Fully Connected Network\n",
    "\n",
    "The goal of this notebook is to introduce the fully connected neuron, activation and building a network with initialized parameters. First, we load an MNIST example for the digit \"5\" and set up some weights for a very simple one-layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "Neural network output: \n",
      " [[  4.87417189]\n",
      " [  8.93202046]\n",
      " [  0.02793288]\n",
      " [ 24.07781799]\n",
      " [ -0.8016074 ]\n",
      " [ -4.42153312]\n",
      " [  8.6226887 ]\n",
      " [  0.21884018]\n",
      " [  9.9636516 ]\n",
      " [-15.61643317]]\n"
     ]
    }
   ],
   "source": [
    "imgfile='number5.jpg'\n",
    "img_pil = Image.open(imgfile)\n",
    "data = np.asarray(img_pil).astype(np.float32).reshape((784,1))\n",
    "# scale from 0-1\n",
    "data=data/255.0\n",
    "plt.imshow(img_pil)\n",
    "plt.show()\n",
    "# random data/label matrices can also be very useful, but here we'll use the digit\n",
    "#data = np.random.random((784,1))\n",
    "weights = np.random.randn(10, 784)\n",
    "biases = np.random.randn(10, 1)\n",
    "output = np.dot(weights,data) + biases\n",
    "print(output.shape)\n",
    "assert output.shape == (10,1), 'Shape is not what we expected'\n",
    "print('Neural network output: \\n',output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define some common activation functions and run them on our output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activated (softmax) output:\n",
      " [[1.99392588e-01]\n",
      " [6.06405806e-09]\n",
      " [2.68896161e-02]\n",
      " [2.12080792e-06]\n",
      " [2.74184100e-10]\n",
      " [3.56942281e-05]\n",
      " [4.90209255e-04]\n",
      " [1.61770278e-06]\n",
      " [2.57050114e-10]\n",
      " [7.73188147e-01]]\n",
      "Sum of activated softmaxed output:  [1.]\n"
     ]
    }
   ],
   "source": [
    "sigmoid = lambda x: 1/(1+np.exp(-x))\n",
    "relu = lambda x: np.maximum(x, 0.0)\n",
    "softmax = lambda x: np.exp(x)/np.nansum(np.exp(x), axis=0)\n",
    "activations_relu = relu(output)\n",
    "activations_sigmoid = sigmoid(output)\n",
    "activations_softmax = softmax(output)\n",
    "print('Activated (softmax) output:\\n',activations_softmax)\n",
    "print('Sum of activated softmaxed output: ', sum(activations_softmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple neural network for digit classification. Random parameters, distributed normally and scaled according to research on ideal scaling for initialization.\n",
    "\n",
    "In this case, we will use a two layer fc, first layer is 784 inputs and 512 outputs, second layer is 512 inputs, 10 outputs. Takes in a 784x1 image (28x28 reshaped) and outputs 10 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights shapes:  [(512, 784), (10, 512)]\n",
      "Biases shapes:   [(512, 1), (10, 1)]\n"
     ]
    }
   ],
   "source": [
    "w_scale=0.01\n",
    "b_scale = 0.0\n",
    "sizes=[784, 512, 10]\n",
    "biases = [0.01*(2*np.random.randn(y, 1)-1) for y in sizes[1:]]\n",
    "weights = [0.00001*(2*np.random.randn(y, x)-1) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "print('Weights shapes: ',[w.shape for w in weights])\n",
    "print('Biases shapes:  ',[b.shape for b in biases])\n",
    "def forward(data, weights, biases, activation=sigmoid):\n",
    "    activations=[data]\n",
    "    for i in range(len(weights)):\n",
    "        feat = np.dot(weights[i], activations[-1]) + biases[i]\n",
    "        # only apply relu activation on penultimate layers\n",
    "        act = activation(feat) if i < (len(weights)-1) or activation==sigmoid else feat\n",
    "        activations.append(act)\n",
    "    return activations[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets run the network on the image data for two different activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output (relu):  [[-0.00774397]\n",
      " [-0.02051468]\n",
      " [-0.01090689]\n",
      " [-0.01056058]\n",
      " [-0.02150136]\n",
      " [-0.00701392]\n",
      " [-0.04108243]\n",
      " [-0.01420905]\n",
      " [-0.01216662]\n",
      " [ 0.01515831]]\n",
      "Output (sigmoid):  [[0.49740954]\n",
      " [0.49424145]\n",
      " [0.49661257]\n",
      " [0.49680188]\n",
      " [0.49398776]\n",
      " [0.49755367]\n",
      " [0.48907388]\n",
      " [0.49577982]\n",
      " [0.4963267 ]\n",
      " [0.50312694]]\n"
     ]
    }
   ],
   "source": [
    "act_fn=relu\n",
    "print('Output (relu):\\n', forward(data, weights, biases, act_fn))\n",
    "act_fn=sigmoid\n",
    "print('Output (sigmoid):\\n', forward(data, weights, biases, act_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Brief aside on regression vs classification. Two of the most popular loss/feedback methods are regression and classification. \n",
    "Regression typically involves metric distance between label and output and results in a scaled floating point value(s).\n",
    "Classification involves finding the maximum (or minimum) output of a network and how much larger it is relative to other outputs.\n",
    "\n",
    "In this case, we wish to classify digits (could also be done with regression, but with some tradeoffs), so we'll use a softmax and select the maximum below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmaxed output:  [[0.10013134]\n",
      " [0.09981462]\n",
      " [0.10005157]\n",
      " [0.10007051]\n",
      " [0.0997893 ]\n",
      " [0.10014577]\n",
      " [0.09930015]\n",
      " [0.09996829]\n",
      " [0.10002297]\n",
      " [0.10070547]]\n",
      "Maximum index:  9\n",
      "\"Probability\":  0.10070547215285618\n"
     ]
    }
   ],
   "source": [
    "softmax = lambda x: np.exp(x)/np.nansum(np.exp(x), axis=0)\n",
    "print('Softmaxed output: ', softmax(forward(data, weights, biases, act_fn)))\n",
    "maxind = np.argmax(softmax(forward(data, weights, biases, act_fn)), axis=0)[0]\n",
    "print('Maximum index: ', maxind)\n",
    "print('\"Probability\": ', softmax(forward(data, weights, biases, act_fn))[maxind][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets load some weights we got from somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output (trained):\n",
      " [[ 0.05441621]\n",
      " [-0.03527451]\n",
      " [-0.00332656]\n",
      " [ 0.37345294]\n",
      " [-0.09257449]\n",
      " [ 0.44667264]\n",
      " [ 0.0884899 ]\n",
      " [ 0.10668931]\n",
      " [ 0.00630097]\n",
      " [-0.01047153]]\n",
      "Softmaxed ()trained) output:\n",
      " [[0.09473242]\n",
      " [0.08660569]\n",
      " [0.08941724]\n",
      " [0.13033303]\n",
      " [0.08178269]\n",
      " [0.14023403]\n",
      " [0.09801592]\n",
      " [0.09981609]\n",
      " [0.09028226]\n",
      " [0.08878063]]\n",
      "\n",
      "Maximum (trained) index:  5\n",
      "\"Probability\":  0.1402340321583118\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "with open('nnet_params.dill','rb') as fp:\n",
    "    params = dill.load(fp)\n",
    "weights = [params[0][0], params[1][0]]\n",
    "biases = [params[0][1], params[1][1]]\n",
    "act_fn=relu\n",
    "print('Output (trained):\\n', forward(data, weights, biases, act_fn))\n",
    "print('Softmaxed ()trained) output:\\n', softmax(forward(data, weights, biases, act_fn)))\n",
    "maxind = np.argmax(softmax(forward(data, weights, biases, act_fn)), axis=0)[0]\n",
    "print('\\nMaximum (trained) index: ', maxind)\n",
    "print('\"Probability\": ', softmax(forward(data, weights, biases, act_fn))[maxind][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila, the right weights can get us good results. We could potentially get as good as 95% with this network (according to the MNIST leaderboard, it requires some data augmentation and other tricks). Notice this particular data example is pretty close to a \"3\", and this is reflected in the softmaxed output (and corresponding \"confidence\" level)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
