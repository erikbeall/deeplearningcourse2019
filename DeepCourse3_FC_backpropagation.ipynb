{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Finding the Parameters via Backpropagation\n",
    "In this notebook, we will walk through the full backpropagation algorithm in two ways, line-by-line and in an organized function. The complexity of keeping track of parameters, derivatives and gradients should demonstrate why we need nicely organized platforms. You will probably think of alternative ways of organizing as we go, if you think of something feel free to speak up and mention these for us all to discuss. It may help you choose a platform to use later on and it will help you develop methods for tracking internals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a few helper functions\n",
    "sigmoid = lambda x: 1.0/(1.0000000000001 + np.exp(-x))\n",
    "relu = lambda x: np.maximum(x, 0.0)\n",
    "softmax = lambda x: np.exp(x)/np.nansum(np.exp(x), axis=0)\n",
    "data = np.random.random((784,1))\n",
    "# derivative of the activation functions\n",
    "sigmoid_delta = lambda x, sigmoid: sigmoid(x)*(1-sigmoid(x))\n",
    "relu_delta = lambda x: np.maximum(x+0.0001, 0.0)/np.maximum(x, 0.0001)-0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's one way to abstract the layers/parameters for a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "def get_params(input_pixels=784, n_hidden=[512], output_classes=10, w_scale=0.01, b_scale=0.0, activations=None):\n",
    "    fc_sizes=[input_pixels]\n",
    "    fc_sizes.extend(n_hidden if type(n_hidden) == type(list()) else [n_hidden])\n",
    "    fc_sizes.append(output_classes)\n",
    "    '''\n",
    "    # How would you abstract your layers?\n",
    "    # One option, build a named collection with lists attached to each\n",
    "    params=collections.namedtuple('params',['weights','biases','activations'])\n",
    "    params.weights=[w_scale*np.random.randn(y, x) for x, y in zip(fc_sizes[:-1], fc_sizes[1:])]\n",
    "    params.biases=[b_scale*np.random.randn(y, 1) for y in fc_sizes[1:]]\n",
    "    params.activations = activations # if activations is not None else [[]]*(len(fc_sizes)-1)\n",
    "    '''\n",
    "    # Another option, layer-indexed params list\n",
    "    # benefit is each layer is an index into the params\n",
    "    params=[]\n",
    "    for i in range(len(fc_sizes)-1):\n",
    "        layer=collections.namedtuple('layer',['weight','bias','activation'])\n",
    "        layer.weights = w_scale * np.random.randn(fc_sizes[i+1], fc_sizes[i])\n",
    "        layer.bias  = b_scale * np.random.randn(fc_sizes[i+1], 1)\n",
    "        layer.activation = activations[i]\n",
    "        params.append(layer)\n",
    "    # Other methods could include a dict or collection of layers, where the input and outputs have specific names\n",
    "    # and each layer object includes the layers that feed into and take output from it (e.g. Caffe)\n",
    "    return params\n",
    "\n",
    "activation_fn=relu\n",
    "delta_activation_fn=relu_delta\n",
    "params = get_params(n_hidden=512, activations=[activation_fn, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a forward() pass function that works with this new collections-based parameter structure. Test it on our data, passing the output through softmax before selecting the max index. For random parameters, recall we should expect the softmax \"probability\" to be 1/N classes = 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing forward with layer-based param structure\n",
      "Maximum index:  2\n",
      "\"Probability\":  0.104\n"
     ]
    }
   ],
   "source": [
    "def forward(data, params, forward_only=True):\n",
    "    activations=[data]\n",
    "    features=[]\n",
    "    for layer in params:\n",
    "        features.append(np.dot(layer.weights, activations[-1]) + layer.bias)\n",
    "        activations.append(layer.activation(features[-1]) if layer.activation is not None else features[-1])\n",
    "    if forward_only:\n",
    "        return activations[-1]\n",
    "    else:\n",
    "        return activations,features\n",
    "    \n",
    "print('Testing forward with layer-based param structure')\n",
    "maxind = np.argmax(softmax(forward(data, params)), axis=0)[0]\n",
    "print('Maximum index: ', maxind)\n",
    "print('\"Probability\": ', np.around(softmax(forward(data, params))[maxind][0], decimals=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief aside on loss function\n",
    "The log-likelihood loss function. I've seen this called cross-entropy (posters, papers), some of the platforms call this \"softmax cross entropy\", which is close enough. \n",
    "\n",
    "True cross-entropy loss is difference between label and output distributions as used with sigmoid while log-likelihood is what it looks like with softmax, and softmax is useful for reporting \"probabilities\" (which they aren't really). Hence, we use softmax+log-likelihood loss for classification here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss function, log of the softmaxed output at the correct class.\n",
    "# Goal of training is to increase it (to 1.0) and decrease all others\n",
    "labels=np.asarray([[8]])\n",
    "ce_loss = lambda labels,output: -np.nansum(np.log(softmax(output))* one_hot(labels, output.shape[0]), axis=0)\n",
    "# turn a label into a one-hot vector (0's for non-label, 1 for the label)\n",
    "one_hot = lambda labels, n_classes: np.eye(n_classes)[np.array(labels).reshape(-1)].transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What might this softmax cross-entropy loss look like in a real training run?\n",
    "Recall softmax produces outputs that sum to 1.0, they look like probabilities from 0.0 to 1.0.\n",
    "Realistically, a random network with 10 classes will produce ten outputs about 1/10 = 0.10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Typical starting softmax cross-entropy loss (p = 0.1):  2.3025850929940455\n",
      "Decent ending softmax cross-entropy loss (p ~ 0.95 or so):  0.05129329438755058\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgAAAAEICAYAAAApw0wKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xd8VGW+x/HPLwVCbwklgST0jpRIERQsKHa99oJlZV0sq656t9x1791i2eK6uquufe2994IiRTooCFINLbSEHkqAJM/94xx0jGmUmTPl+369eJGZc+bM73lO+845Z86Ycw4RERFJLElBFyAiIiKRpwAgIiKSgBQAREREEpACgIiISAJSABAREUlACgAiIiIJKKwBwMzONrPVZrbDzPqF870kOpjZFWY2Oeg6wsXMPjCzy4Ou40CY2ZNmdnvQdQTBzHLNzJlZiv845ubfoTCzVmY20cyKzezvZvY/ZvZYNeOvMLMTIlljvDlcy5yZZfv7zuTDX6WnxgBgZsPMbIqZbTOzzWb2hZkdWcvp3w1c75xr6Jz7UgtXePgLW6eg64gmZva5mY05xGn83syeDX3OOXeyc+6pQ6vuoGo55PZIQs6/q4GNQGPn3C3OuTudc2Gvo7J1p5JxupvZZ/6+ZZmZnV1h+Bj/+R1m9qGZZYYMu9jM1pnZcjMbEfJ8R39/Fbad5oGq7TJXcf/onFvl7zvLwlVbtQHAzBoD7wL/ApoDWcAfgD21nH4OsOBQCgza/hQXy+KhDZGmPpM4kQN846Lsjm/++vUW3v6lOV5QedbMuvjDhwN3Amf6w5cDL4S89s9Af+DnwP0hk/4ncPPh2mmaJ35PlTvnqvwH5AFbqxmeBNwGrAQKgaeBJkBdYAfggJ3At8AzQDmw2x/2SyDXH+dKYDWwBRgLHAnMA7YC94e8X0fgM2ATXqp9DmgaMmwz0N9/nOmPM6KK2tsBrwNF/vTu95+/AvgC+Ic/vduraqc/fhrwrD+NrcBMoFXItPKBYrwF+JJq+rIb8In/nouB80OGPQk8ALznT2s60NEfNjGkn3cAFwAjgALgV8B64Bl/3J8Cy/z3eBvIDHkPB9zg17sR+Jvf7rr++L1Dxm3pz8eMStpxBTA55PFRfp9s8/8/qsK4P+ofoBMwwX/NRuClKvqs0r4H7gDKgBK/T/bP2/vwlrPtwGzg6JBp/R541Z/eduB6YC+wz5/GXH+8z4ExoW3FO9K1xW/DySHTbO/Pn2JgnD8Pn62iLc3wNoZF/rTeBdr6wyptTyXTGAZM8ftiNXBFTctPLfvlZbxlvhgv0OfVtB75w34CLPTb8xGQU932pkJbqnwt3rI6FljqD38AMH9Ysj8/NvrL1nX++CkxPP+a+P1fhLcNug1IqqkN/nzfh7cc7wBO8OfnsyHvOdqf5ibgt8AK4ISQ7fuv8bbfm/zloLk/LNfv18uBVX5//9YfNopK1p0Kbe3lD7OQ5z4G/uT/fTfwQMiwTP/9OuKt41NDtgG7/L/PBR6pxbJ1Bd42/l9425hFwPEhwz/359kXeNu5Tv48eBxYB6zB2y8kH+gyF7IdXoi3XH2DF2Sq2z+mhPTB23jb42XAT2u7nlbZFzV0VGN/xj8FnAw0q2QlXQZ0ABribQieqbCidgp5/N3CVWEhesifkSfirSRv4u1ksvB2uMNDdgwj8XZKGXgr572VdGx9vI3G3VW0KxmYi7eTb+C/97CQhaMUL1mmAPWqayfwM+Ad/z2TgQF+vzXA26B29cdrA/Ssop4GeCv8lf579vcXpp4hK/JmYKA//DngxWr6eYTfhr/4fVUPOM6fZn//uX8BEytMYzxe2s4GlvD9hvJB4C8h494IvFPNyjXZ/7s53kZptF/3Rf7jFtX1D17S/y3eBui7eVPJe1Xa95WtdP5zl/rvnQLcgheO0kJWoH3AWf771qPCxrKKHcg+vOUuGbgGWMv3O6OpeBuGOngb9+0Vpxcy3RbAOX5bGgGvAG9W9r5VvD4bb8W/CEj1p9e3lstPTf1SApzit/EuYFot1qOz8NaZ7v50bwOm1LRBqs1r8ZbVd4GmfruLgFH+sLF4G/R2eMvfeKoPALEw/57G+7TcCG+buQS4qpZteBK4vcKO4ln/7x54O5tj8LYJ9+BtN/YHgJuAaUBbf/jDwAsVtt2P4q0rR+AdGe5e8X2qaG9vfhwAPgHe8P/+O/BgyLAs//3OxFs/l/h1nY4X/BsCXwEtarF8XeG38xd+X1+AFwT2h5vP8UJNT7zlLxVvn/Qw3nLeEpgB/Owglrnz8ALEkYDh7dNy/GErqHz/uH86E/C2xWlAX7zl/via1tNq+6IWndXdX4gK/E57m+8/4X4KXBsyble8hXF/wbUNAFkhz20CLgh5/BpwUzUbii8rPPc28DXeEYS6VbxuiN95KVUsHKsqPFdlO/HCwRSgT4XXNMBL8ucA9Wro4wuASRWeexj4v5CV+LGQYacAiypsECsGgL34G3H/uceBv4Y8bui3ITdkGqNChl8LfOr/PQgvoOz/1DGLkCMUlfTf/gAwGphRYfhUf5wq+wdvg/cI/ieoavqt0r6vuNJV8/otwBEhK9DECsN/T80BYFnIsPp+P7bG26CXAvVDhj9bcXrV1NYX2FLb9gC/wd94VjKs2uWnFv0yLmRYD2B3LdajD/B3Uv7jJGAXtTgKUNNr/T4eFjL8ZeDX/t+fAWNDhp1I9QEgqucf3sZ8D9Aj5LmfAZ/X1IaQeV9VAPhffhgEG+BtN/YHgIX88JNxG77f7uX679M2ZPgM4MKq1p0K7UrF+7T8S//vE/33/sgffjzeB5Y+eAHjYbxPyBeFDJ+Gt1PsixdersLb9o3H+wDYq4r3voKQkBRS++iQefXHkGGt/HlQL+S5i4DxB7HMfQTcWEVdK6giAOCFizKgUcjwu4Ana1pPq/tX47kN59xC59wVzrm2eIdtMoF7/cGZeIeP9lvpF9uqpulWsCHk792VPG4IYGYtzexFM1tjZtvxVsj0CtN61K/zX865qq5VaAesdM6VVjF8dYXH1bXzGbyZ+qKZrTWzv5pZqnNuJ96OfSywzszeM7NufjsW+Be27DCzo/HO0w0ys637/wGX4G2I9lsf8veu/X1SjSLnXElVbXDO7cALW1lVtHul/xqcc9PxTjEM99vQCS9o1aRiv+2fblZ1/YO3UTBght9XP6li+pX2fVXFmNktZrbQv+hoK95hvdDlp+J8r43v5otzbpf/Z0O8tm8Oea7a6ZtZfTN72MxW+sv2RKDpAVzM1A7vUG2NdVJh+alFv1R8bZp/Hra69SgHuC9ked6MN0+zKhn3YF5bVXsy+fFyXJ1on3/peEcgKm5/Ku2LCm2oyQ/6yl8nN4UMzwHeCJkPC/F2QqHb9wPdLu1/r/1H2071p3ELXpAr8Id/Cvwf3gfAlXg7x+LQ4c65wc654XjBIA8v7DyDt4P/E1Dltx2ANc7fU/q+2975Qud1Dl5IWRfSFw/jHQmAA1vmalpPq7J/eSyu8D7VrRP719MqHdDFDc65RXid3Mt/ai1e5+y3PzVvoHKuiudr6y5/Gn2cc43xDl3a/oFm1hAvnDwO/N7MmlcxndVAdjWdU7HOKtvpnNvnnPuDc64H3vnu04DLAJxzHznnRuIl50V44QTnXE/nXd3Z0Dk3ya9ngnOuaci/hs65a2rVKwfRBjNrgHeYcU3IOO0qtHFtyOOn8Pp7NPBqhXBRlYr9tn+6a6Da/lnvnPupcy4T79POg5V9y6G6vqdC+/2g9SvgfLxTWU3xDvtZ6CQrvkUt2liVdUBzM6sf8ly7qkbG2wB2BQb5y/Yx+0uvZS2r8c6PHpBa9kt171nVerQa7xBp6DJdzzk3pZbTPdjXruPHy/HBiJb5txHvU3fF7c+aSsY9UD/oK7+tLSrUdHKF+ZDmnKvNe9e47jjn5jnnhjvnWjjnTsI7xTojZPgDzrnOzrmWeEEgBZgfOg0zM7yLAG/AC0vJzrmVeKcF+lTz9ln+a/eruL0LrX813hGA9JB+aOyc6+kPP5Blrrr1tLo+W4u3PDaq8D6HtBzU9C2Abv6ng7b+43Z4hz6m+aO8APzCzNr7O9878S7YquqT9Qa8mXywGuGdN9pqZlnAf1cYfh8w23lfc3kP79qCyszAm2l/NrMGZpZmZkOred8q22lmx5pZbz/pb8dbWcvM+/7tGf6Odo9fd1VXpr4LdDGz0WaW6v870sy619Qhvtr06/PAlWbW18zq+m2Y7pxbETLOf5tZM38+3wi8FDLsGeBsvBDwdC3reh+vXRebWYqZXYB3aOrd6vrHzM7bv8zhHY52VNJ3VfW9P7hinzTCC21FQIqZ/S/etRrV2QDkHsxVwP5GaBZeEK1jZkPwzldWpRHe0a6tfnD9v0pqqW4ePwecYGbn+33dwsz61qLUg+mX/apbjx4CfmNmPQHMrImZnbf/heZ9Le73VUy32tfW4GXgBjNra2bN8C5iO2DRMv+cdzX7y8AdZtbIzHKAm/GOfh6qV4HTzPuqdx3gj/xwn/CQ/745AGaWYWZn1nLaNa47ZtbHX2bqm9mteB8EnvSHpZlZL/Nk450SvM85t6XCZMbgnQb+Cu/oRT0z6wEci3eKoSot8ZaTVH/Z6o63vfoR59w6vAsU/25mjc0sybyvGw73RzmQZe4x4FYzG+C3rdP+/qWaZcQ5txrvdOddft/0wTvl8Vw171WjmjZsxXjnf6eb2U68Hf98vLQL8ATejmEi3tWnJXgXz1XlLuA28w6j3HoQ9f4B7yK2bXg7+Nf3D/AXzFF4h5TBW0n6m9klFSfir1Sn4x3KXoV3WOmCat63una2xluRtuMdIpuAt3Im4fXTWrxDmMPxzqv/iH9Y50TgQn/89Xx/AV9t/B54yu/X86t4j0+B3+El6XV4KfTCCqO9hXcV+Fd4/ft4yOsLgDl4O+NJtSnKObcJ71P5LXgr5y+B05xzG6m+f47EW+Z24J1quNE5t7ySt6iq78ELg+ea2RYz+yfeqYIP8C4eWok3D2s65P+K//8mM5tTmzZXcAneefJNeFcNv0TVX6G9F+9c50a89ezDCsMrtucHnHOr8M7t34LXn1/hXZhVk4Ppl/3vWeV65Jx7A28ZftG8Q+Lz8S4k3q8d3lXWlU23ptdW51G/TXPxltfXqx+9WtEy/36OdwouH++K/+fxtkmHxDm3AO+K9efxtglb8A+xh9T8NvCxmRX77RpUy8nXZt0Z7b9vId45/ZEhp23T/Lp24AXNqXjbr++YWTreB5Xf+e0pxfv2zmd44aW6fdF0oDPe/LoDONffXlXlMrxTMd/g9dOreIEFDmCZc8694r/f83j71zfxLhyEmvePF+FdF7AWeAPvGrFPqqm5RvuvFJUEZ2YO6OycW1bNOE8Aa51zt0WusvhhZi/hXXxX8dNhQvGP7rzinBsSdC0HQvMvPpjZFXgX5Q0Lupagxe8NDuSwMrNc4L8IOSog1fNP43T0DxmOwvsK05tB1xU051xBLOz8Nf8k3uluZ1IjM/sT3ndm76riULxUrjXe4cAWeIdWr3HOfRlsSXIANP8krukUgIiISALSKQAREZEEpFMAEZCenu5yc3ODLkNEJKbMnj17o3MuI+g64pUCQATk5uYya9asoMsQEYkpZlbTnRzlEOgUgIiISAJSABAREUlACgAiIiIJSAFAREQkASkAiIiIJCAFABERkQSkACAiIpKAFACi2Ntz1/LSzFVBlyEiInFIASCKvTN3LX/7aDF7SsuCLkVEROKMAkAUGz04h4079vLh/PVBlyIiInFGASCKDeuUTm6L+jw7TXfDFBGRw0sBIIolJRmXDs5h5ootLFy3PehyREQkjigARLlzB7SlbkoSz+gogIiIHEYKAFGuaf06nH5EJm9+uYbtJfuCLkdEROKEAkAMGD04h117y3hjzpqgSxERkTihABADjmjXlD5tm/D01BWUl7ugyxERkTigABAjrhyay7dFO5mwpCjoUkREJA4oAMSI0/pk0rpxGo9MzA+6FBERiQMKADEiNTmJK4bmMjV/E/PXbAu6HBERiXEKADHkooHZNKiTzGOTdBRAREQOjQJADGlSL5ULjszm3XnrWLdtd9DliIhIDFMAiDFXDs2l3Dme/GJF0KWIiEgMUwCIMe2a1+eU3m14fvoqtu3WjYFEROTgKADEoGtGdKR4TylPT1kRdCkiIhKjFABiUM/MJpzQvSWPf7GcnXtKgy5HRERikAJAjLru2E5s3bWP56brR4JEROTAKQDEqH7ZzTi6czqPTFxOyb6yoMsREZEYowAQw64/thMbd+zhxRmrgi5FRERijAJADBvUoQUDc5vz0IR8HQUQEZEDogAQ4246oTPrt5fw/HQdBRARkdpTAIhxR3VK56iOLXjw82X6RoCIiNSaAkAIM2tnZuPNbKGZLTCzGysZx8zsn2a2zMzmmVn/IGoNdetJXdm4Yy9P6r4AIiJSSwoAP1QK3OKc6w4MBq4zsx4VxjkZ6Oz/uxr4d2RL/LH+2c04oXtLHp7wLdt26e6AIiJSMwWAEM65dc65Of7fxcBCIKvCaGcCTzvPNKCpmbWJcKk/csuJXdleUsojk74NuhQREYkBCgBVMLNcoB8wvcKgLGB1yOMCfhwSMLOrzWyWmc0qKioKV5nf6d6mMacfkckTk1dQuL0k7O8nIiKxTQGgEmbWEHgNuMk5t73i4Epe4n70hHOPOOfynHN5GRkZ4SjzR24Z2YXS8nL+MW5JRN5PRERilwJABWaWirfzf84593oloxQA7UIetwXWRqK2muSmN2D04FxemrmaxeuLgy5HRESimAJACDMz4HFgoXPunipGexu4zP82wGBgm3NuXcSKrMENx3eiYd0U7nx/YdCliIhIFFMA+KGhwGjgODP7yv93ipmNNbOx/jjvA/nAMuBR4NqAaq1U0/p1uOH4zkxYUsTEJeG/9kBERGJTStAFRBPn3GQqP8cfOo4DrotMRQdn9JAcnpq6gjvfX8jQTukkJ1XbJBERSUA6AhCH6qYk8+tR3Vm0vpgX9ENBIiJSCQWAOHVK79YM7tCcuz9ezJade4MuR0REoowCQJwyM/5wRi+KS0q5++PFQZcjIiJRRgEgjnVt3YjLh+Ty/IxVzF+zLehyREQkiigAxLmbRnamRYM6/O9b8ykv/9H9ikREJEEpAMS5xmmp/GpUN+as2sors1fX/AIREUkICgAJ4NwBbRnYvjl3vr+IjTv2BF2OiIhEAQWABGBm3Hl2L3btLeVP734TdDkiIhIFFAASRKeWjbhmRCfe+motE3SHQBGRhKcAkECuHdGRDukNuO3Nr9m9tyzockREJEAKAAkkLTWZO87uzerNu7nnE90bQEQkkSkAJJghHVtwyaBsHpu8nNkrNwddjoiIBEQBIAH95pTuZDapx3+/Oo+SfToVICKSiBQAElDDuin85Zw+5Bft5B+fLAm6HBERCYACQIIa1jmdiwZm8+ikfGav3BJ0OSIiEmEKAAnsf07pRpsm9bj55a/Yuac06HJERCSCFAASWKO0VO45/whWbd7F7e/pBkEiIolEASDBDerQgp8d05EXZqzmk282BF2OiIhEiAKAcPPILvRo05hfvzaPomL9VoCISCJQABDqpCRx74V92bGnlFtfmaufDRYRSQAKAAJAl1aN+N1pPZiwpIjHJucHXY6IiISZAoB855JB2ZzcqzV//XAxX63eGnQ5IiISRgoA8h0z48/n9KFV4zR+/sIctpfsC7okEREJEwUA+YEm9VL518X9WLe1hF++Mg/ndD2AiEg8UgCQH+mf3YxfjerGhwvW8/jk5UGXIyIiYaAAIJUac3R7RvVszV0fLGLGcv1qoIhIvFEAkEqZGX87rw/Zzetz/fNzKCwuCbokERE5jBQApEqN0lL596X92V6yj+uf+5K9peVBlyQiIoeJAoBUq1vrxvzlnD7MWLGZP72r3wsQEYkXKUEXINHvzL5ZLFi7nUcm5tMzszEXDswOuiQRETlEOgIgtfKrUd04unM6v3trPrNXbgm6HBEROUQKAFIryUnGvy7qR2bTevzsmVkUbNkVdEkiInIIFACk1prWr8Pjl+exp7ScMU/NYsee0qBLEhGRg6QAIAekU8tGPHBxf5YW7uDGF76kTL8cKCISkxQAQpjZE2ZWaGbzqxg+wsy2mdlX/r//jXSN0eCYLhn8/oyefLqokDveWxh0OSIichD0LYAfehK4H3i6mnEmOedOi0w50Wv04Bzyi3bwxBfLade8HlcObR90SSIicgAUAEI45yaaWW7QdcSK207twdqtu/nju9/Qpkkao3q1CbokERGpJZ0COHBDzGyumX1gZj2rGsnMrjazWWY2q6ioKJL1RUxyknHvBf3o264pN774lb4eKCISQxQADswcIMc5dwTwL+DNqkZ0zj3inMtzzuVlZGRErMBIq1cnmccuy6NNkzTGPDWTZYXFQZckIiK1oABwAJxz251zO/y/3wdSzSw94LIC16JhXZ76yUCSk5K47PEZrN26O+iSRESkBgoAB8DMWpuZ+X8PxOu/TcFWFR1yWjTgqZ8cSXFJKZc9MYMtO/cGXZKIiFRDASCEmb0ATAW6mlmBmV1lZmPNbKw/yrnAfDObC/wTuNA5py/C+3pmNuHRy/NYtXkXVzw5UzcKEhGJYqb9V/jl5eW5WbNmBV1GxHy8YD3XPDeHI3Ob8eSVA0lLTQ66JBGJQWY22zmXF3Qd8UpHAOSwO7Fna/5+3hFMX76Za5+bw97S8qBLEhGRChQAJCzO6pfFHWf15rNFhfzipa8oLVMIEBGJJroRkITNxYOy2bW3lNvfW0hKsnHP+X1JTrKgyxIRERQAJMzGHN2BvWXl/PXDxSSb8bfzjlAIEBGJAgoAEnbXjuhEebnj7o+XkJRk/OWcPgoBIiIBUwCQiLj+uM6UljvuHbeU8nKnIwEiIgFTAJCIuemELiSb8fdPllBa7rjn/CNISdZ1qCIiQVAAkIj6+fGdSUlO4i8fLqK0vJx7L+hHnRSFABGRSFMAkIi7ZkRHUpON299bSMm+2Tx4SX/dLEhEJML00UsCMeboDtx+Vi/GLy7kyv/MZKduGywiElEKABKYSwfncM/5RzBjxWYufXw6W3fpB4RERCJFAUACdXa/tjx4SX8WrNnO+Q9PZf22kqBLEhFJCAoAEriTerbmyZ8cydqtJZzz7ynkF+0IuiQRkbinACBR4aiO6bx49WBK9pVx7kNTmbt6a9AliYjENQUAiRq9sprw6jVH0aBuMhc+Mo3PFm0IuiQRkbilACBRpX16A1675ig6tWzIT5+ezYszVgVdkohIXFIAkKjTslEaL149mGGd0vn1619z90eLcc4FXZaISFxRAJCo1KBuCo9dnscFee24f/wybnzxK0r2lQVdlohI3NCdACVqpSYn8edzepOTXp+/friYtVt388hleTRvUCfo0kREYp6OAEhUMzOuHdGJ+y/ux7w12zjzgcks2VAcdFkiIjFPAUBiwml9Mnnp6sGU7Cvnvx6cwvjFhUGXJCIS0xQAJGb0y27GW9cNJbt5fa56ciaPTszXxYEiIgdJAUBiSmbTerx6zRBO6tmaO95fyM0vz9XFgSIiB0EBQGJO/TopPHhJf24Z2YU3vlzD+Q9PZd223UGXJSISUxQAJCaZGT8/vjOPXpZHftFOTv/XZKZ+uynoskREYoYCgMS0kT1a8eZ1R9GkXiqXPj6dxybpugARkdpQAJCY16llI966fhgju7fi9vcWcv0LX7JjT2nQZYmIRDUFAIkLDeum8O9L+/Prk7vx4fz1nHH/ZBav1/0CRESqogAgccPMGDu8I8+NGURxSSlnPjCZ12YXBF2WiEhUUgCQuDO4Qwveu2EYfds15ZZX5nLrK3PZtVenBEREQikASFxq2SiN58YM5objO/PanALOuP8LnRIQEQmhACBxKznJuHlkF569ahBbd+3jjPsn8+y0lfqWgIgICgCSAIZ2SueDG49mUIcW3PbmfMY+O5utu/YGXZaISKAUACQhZDSqy5NXHMlvT+nOZ4sKGXXvJKZ8uzHoskREAqMAEMLMnjCzQjObX8VwM7N/mtkyM5tnZv0jXaMcvKQk46fHdOD1a4ZSv04ylzw2nbveX8ieUv2WgIgkHgWAH3oSGFXN8JOBzv6/q4F/R6AmOcx6t23CuzcM46KB2Tw8MZ+zH5iiCwRFJOEoAIRwzk0ENlczypnA084zDWhqZm0iU50cTvXrpHDn2b159LI8Nmwv4fT7J/PoxHzKy3WBoIgkBgWAA5MFrA55XOA/JzFqZI9WfPSLYxjeJYM73l/IRY9OY/XmXUGXJSISdgoAB8Yqea7Sj4xmdrWZzTKzWUVFRWEuSw5FesO6PDJ6AH89tw/frN3OSfdO1NcFRSTuKQAcmAKgXcjjtsDaykZ0zj3inMtzzuVlZGREpDg5eGbG+Xnt+PAXxzAgpxm3vTmf0Y/PoGCLjgaISHxSADgwbwOX+d8GGAxsc86tC7ooOXyymtbj6Z8M5PazevHlqi2c9I+JPDN1ha4NEJG4owAQwsxeAKYCXc2swMyuMrOxZjbWH+V9IB9YBjwKXBtQqRJGZsalg3P46BfH0D+nGb97awEXPTqN5Rt3Bl2aiMhhYzrPGX55eXlu1qxZQZchB8E5x8uzVnP7ewvZW1rOTSd0YczR7UlNVnYWCTczm+2cywu6jnilrZhINcyMC47MZtzNwxnRNYO/fLiIsx74gq8LtgVdmojIIVEAEKmFVo3TeHh0Hg9d2p/C4j2c+cBk/vjON+zco58ZFpHYpAAgcgBG9WrDp7cM5+JB2fxnynJG3jOBjxesD7osEZEDpgAgcoAap6Vy+1m9eXXsUTRKS+XqZ2Yz5qmZuoGQiMQUBQCRgzQgpxnv3jCM35zcjS+WbWLkPybwwPhl+nEhEYkJCgAihyA1OYmfDe/IuFuGc0znDP720WJOvncSk5bq7o8iEt0UAEQOg6ym9Xjksjz+c+WRlDnH6MdncM2zs3UnQRGJWgoAIofRsV1b8tFNx3DriV0Yv7iQE+6ZwH3jllKyT6cFRCS6KACIHGZpqclcf1xnPrtlBMd3b8U/xi2qMWIcAAASzElEQVTh+L9P4L156/QDQyISNRQARMIks2k9Hri4Py/8dDCN0lK47vk5XPDINOav0U2ERCR4CgAiYTakYwveu+Fo7jy7N8sKd3D6/ZP571fmUri9JOjSRCSBKQCIREByknHxoGzG3zqCnx7dgTe/WsOIuz/nn58uZdde3U1QRCJPAUAkgprUS+V/TunOuJuHM7xLBvd8soRj7/6cl2eupkw/OSwiEaQAIBKAnBYN+PelA3h17BDaNKnHL1+bx6n/nMT4xYW6UFBEIkIBQCRAebnNeePao7j/4n7s2lvGlf+ZycWPTmdewdagSxOROKcAIBIwM+O0PpmMu3k4vz+9B4s3FHPG/V9w3XNzyC/aEXR5IhKnTIcbwy8vL8/NmjUr6DIkRhSX7OOxSct5bFI+JaXlnJ/XlhuO70ybJvWCLk0kosxstnMuL+g64pUCQAQoAMjB2LhjD/d/toznp68Cg9GDc7h2REdaNKwbdGkiEaEAEF4KABGgACCHomDLLu4bt5TX5hSQlprMlUNzufrojjSpnxp0aSJhpQAQXgoAEaAAIIfDssId3DtuCe/OW0ejtBTGDOvAlcNyaZymICDxSQEgvBQAIkABQA6nheu2849PlvDxNxtoUi+VMcPac8XQXBopCEicUQAILwWACFAAkHCYv2Yb945bwriFhTSt7wWBy49SEJD4oQAQXgoAEaAAIOE0r2Ar945bymeLCmlSL5Wr/CDQpJ6CgMQ2BYDwUgCIAAUAiYSvC7Zx36dLGbdwA43qpnDF0Fx+MrQ9zRrUCbo0kYOiABBeCgARoAAgkbRg7Tbu/2wZH8xfT/06yVwyKJufHt2Blo3Tgi5N5IAoAISXAkAEKABIEJZsKObB8ct4e+5aUpKTOG9AW352TEeyW9QPujSRWlEACC8FgAhQAJAgrdy0k4cm5PPa7ALKnOO0Pm24ZkRHurVuHHRpItVSAAgvBYAIUACQaLB+WwmPT87n+emr2Lm3jGO7ZvCz4R0Z1L45ZhZ0eSI/ogAQXgoAEaAAINFk6669PDN1JU9OWcGmnXs5ol1Txh7TgRN7tiY5SUFAoocCQHgpAESAAoBEo5J9Zbw6u4BHJ+WzctMuclrUZ8yw9pw7oB316iQHXZ6IAkCYKQBEgAKARLOycsfHC9bz8MR8vlq9lWb1U7l0cA6jh+TQspG+OSDBUQAILwWACFAAkFjgnGPmii08NimfTxZuIDUpiTP6ZnLVsPZ0b6MLBiXyFADCKyXoAkQkOpgZA9s3Z2D75izfuJMnJi/n1dkFvDq7gKM6tuCqYe05tmtLknSdgEhc0BGACNARAIlVW3ft5cWZq3lqygrWbSsht0V9Lj8ql3MHtNVvDkjY6QhAeCkARIACgMS6fWXlfLRgPU9MXs6cVVtpWDeFcwe05bIhOXTIaBh0eRKnFADCSwEghJmNAu4DkoHHnHN/rjB8BPAWsNx/6nXn3B9rmq4CgMSTuau38tSUFbwzby37yhzDu2Rw+VE5DO/SUl8jlMNKASC8FAB8ZpYMLAFGAgXATOAi59w3IeOMAG51zp12INNWAJB4VFS8h+enr+L5GSvZsH0P2c3rc+ngbM4b0E4/QCSHhQJAeCUFXUAUGQgsc87lO+f2Ai8CZwZck0jUymhUlxtP6MzkXx3HAxf3p3XjNO58fxGD7/qUW1+Zy9zVW4MuUUSqoW8BfC8LWB3yuAAYVMl4Q8xsLrAW72jAgsomZmZXA1cDZGdnH+ZSRaJHanISp/Zpw6l92rBo/XaenbaSN+as4dXZBfTOasIlg7I5o28m9etocyMSTXQKwGdm5wEnOefG+I9HAwOdcz8PGacxUO6c22FmpwD3Oec61zRtnQKQRFNcso83v1zDs9NWsXhDMY3qpnB2/ywuHpStHyGSWtMpgPBSJP9eAdAu5HFbvE/533HObQ/5+30ze9DM0p1zGyNUo0hMaJSWyughuVw6OIfZK7fw3PRVvDhzNU9PXUn/7KZcNDCb0/pk6pbDIgHSEQCfmaXgXQR4PLAG7yLAi0MP8ZtZa2CDc86Z2UDgVSDH1dCJOgIgAlt27uW1OQU8P2MV+UU7aZSWwll9s7hwYDt6ZjYJujyJQjoCEF46AuBzzpWa2fXAR3hfA3zCObfAzMb6wx8CzgWuMbNSYDdwYU07fxHxNGtQhzFHd+CqYe2ZuWILL8xYxUuzVvPMtJX0zmrCBUe244y+mTTWDYZEIkJHACJARwBEKrdt1z7e+LKAF2euZtH6YtJSkzilVxvOP7Idg9o3x0z3FUhkOgIQXgoAEaAAIFI95xzzCrbx0qzVvPPVWor3lJLToj7nDWjLOQPa0qZJvaBLlAAoAISXAkAEKACI1N7uvWW8//U6Xpm9mmn5m0kyGNY5g3MHtOXEHq1IS9WFg4lCASC8FAAiQAFA5OCs2rSLV2ev5rU5a1izdTeN0lI4rU8m5w7Ion92M50iiHMKAOGlABABCgAih6a83DEtfxOvzC7gw/nr2b2vjPbpDfivflmc1S+Lds3rB12ihIECQHgpAESAAoDI4bNjTykffL2O1+YUMC1/MwAD2zfnv/plcXLvNjSpp28RxAsFgPBSAIgABQCR8CjYsos3v1zD63PWkL9xJ3VSkhjZvRVn9s1kRNeW1EnRz53EMgWA8FIAiAAFAJHw2v8tgje+XMM7c9eyaedemtZP5ZTebTirbxZ5Oc1I0k8VxxwFgPBSAIgABQCRyNlXVs7kZRt588s1fLxgA7v3lZHZJI3T+2Zy5hFZdG/TSBcPxggFgPBSAIgABQCRYOzcU8on32zgra/WMHHpRsrKHZ1bNuSMIzI5o28mOS0aBF2iVEMBILwUACJAAUAkeJt27OH9+et556u1zFjhXTzYp20TTu+Tyal92pDZVDcbijYKAOGlABABCgAi0WXt1t28N28d78xby7yCbQAMyGnGaX3acGrvNrRsnBZwhQIKAOGmABABCgAi0WvFxp28O28t785bx6L1xZjBwNzmnNanDaN6tSGjUd2gS0xYCgDhpQAQAQoAIrFhWWEx78xdx3tfr2NZ4Y7vwsCpfdowqmdrHRmIMAWA8FIAiAAFAJHYs2RDMe/OW8f7IWHgyJzmnNy7NaN6tdYPFEWAAkB4KQBEgAKASGxbuqGY975ex4fz17NofTEA/bKbcnKv1pzcq41uRRwmCgDhpQAQAQoAIvHj26IdfDh/PR/MX8f8NdsB6NGmMaN6eUcGOrdsqPsMHCYKAOGlABABCgAi8Wn15l18OH89Hy1Yz+xVW3AO2qc34MSerTipZ2v6tm2qOxAeAgWA8FIAiAAFAJH4V7i9hI+/2cBHC9Yz9dtNlJY7Wjaqy8gerTixZ2uGdGih3yY4QAoA4aUAEAEKACKJZdvufYxfVMhHC9YzYUkRu/aW0ahuCsO7ZnBiz9aM6JpB4zT9amFNFADCSwEgAhQARBJXyb4yJi/dyCffbODTRRvYuGMvKUnG4A4tOKF7S07o0Yq2zXQRYWUUAMJLASACFABEBKCs3PHlqi18snAD477ZwLdFOwHo1roRI3u04vjureiT1UTXDfgUAMJLASACFABEpDL5RTv4dGEh4xZuYNbKLZSVO9Ib1uW4bhkc160VR3dOp0HdlKDLDIwCQHgpAESAAoCI1GTrrr18vriIcQs3MGFJEcUlpdRJTmJQh+Yc360lx3VrRXaLxDpVoAAQXgoAEaAAICIHYl9ZObNWbOHThRv4bFEh+Ru9UwUdMxpwXLeWHNutJXk5zeP+WwUKAOGlABABCgAiciiWb9zJZ4sKGb+okOnLN7GvzNGwbgrDOqVzbLcMRnRtSas4/J0CBYDwUgCIAAUAETlcduwpZcqyjYxfXMTniwtZt60EgO5tGjOiawYjumTQP6cZqcmxf3RAASC8FAAiQAFARMLBOcfiDcV8vriI8YsKmb1yC6XljkZ1UxjaKZ3hXTM4pksGWU1j84eLFADCSwEgAhQARCQSikv28cWyTUxYUsiExUWs9Y8OdG7ZkGO6ZDC8SwYD2zcnLTU54EprRwEgvBQAIkABQEQizTnHssIdfL64iIlLi5i+fDN7S8upm5LEwPbNGd7FOzoQzT9epAAQXgoAEaAAICJB2723jGnLNzFxSRGTlm5kWeEOAFo1rsuwThkc0yWdoZ3SSW9YN+BKv6cAEF6Je4cJEZEEUq9OMsd2bcmxXVsCsHbrbiYv3cjEpUV8umgDr80pALyfNj66sxcGjsxtTr06sXG6QA6cjgBEgI4AiEg0Kyt3LFi7jUlLNzJ56UZmr9zC3rJy6qQkkZfTjKGd0hnWKZ1eWU1IjuBtinUEILwUACJAAUBEYsmuvaXMWL6ZyUs38sW3m1i4bjsAjdNSGNKxBUM7pXNUx3Q6ZjQI6/UDCgDhpVMAIiLyA/XrpDCia0tG+KcLior3MOXbjXyxbCNfLNvERws2AN71A0M7pn8XCjJj9OuGiUpHAEKY2SjgPiAZeMw59+cKw80ffgqwC7jCOTenpunqCICIxAvnHCs37WLKt5v44tuNTP12E5t37gUgt0V9hviBYEiHFmQ0OrQLCnUEILwUAHxmlgwsAUYCBcBM4CLn3Dch45wC/BwvAAwC7nPODapp2goAIhKvyssdi9YXMzV/E1O/3cj0/M0U7ykFvPsPPHJZHu3TGxzUtBUAwkunAL43EFjmnMsHMLMXgTOBb0LGORN42nmpaZqZNTWzNs65dZEvV0QkeElJRo/MxvTIbMxVw9pTWlbOgrXbmfLtJmYs30SbJvH3GwXxQgHge1nA6pDHBXif8msaJwv4UQAws6uBqwGys7MPa6EiItEqJTmJI9o15Yh2TblmRMegy5FqxP6vRRw+lV3KWvH8SG3G8Z507hHnXJ5zLi8jI+OQixMRETmcFAC+VwC0C3ncFlh7EOOIiIhEPQWA780EOptZezOrA1wIvF1hnLeBy8wzGNim8/8iIhKLdA2AzzlXambXAx/hfQ3wCefcAjMb6w9/CHgf7xsAy/C+BnhlUPWKiIgcCgWAEM659/F28qHPPRTytwOui3RdIiIih5tOAYiIiCQgBQAREZEEpAAgIiKSgHQr4AgwsyJg5UG+PB3YeBjLiRWJ2G61OXEkYrsPps05zjndSCVMFACinJnNSsR7YSdiu9XmxJGI7U7ENkc7nQIQERFJQAoAIiIiCUgBIPo9EnQBAUnEdqvNiSMR252IbY5qugZAREQkAekIgIiISAJSABAREUlACgBRysxGmdliM1tmZr8Oup5wMbN2ZjbezBaa2QIzu9F/vrmZfWJmS/3/mwVd6+FmZslm9qWZves/ToQ2NzWzV81skT/Ph8R7u83sF/6yPd/MXjCztHhss5k9YWaFZjY/5Lkq22lmv/G3b4vN7KRgqk5sCgBRyMySgQeAk4EewEVm1iPYqsKmFLjFOdcdGAxc57f118CnzrnOwKf+43hzI7Aw5HEitPk+4EPnXDfgCLz2x227zSwLuAHIc871wvul0QuJzzY/CYyq8Fyl7fTX8QuBnv5rHvS3exJBCgDRaSCwzDmX75zbC7wInBlwTWHhnFvnnJvj/12Mt0PIwmvvU/5oTwFnBVNheJhZW+BU4LGQp+O9zY2BY4DHAZxze51zW4nzduP96mo9M0sB6gNricM2O+cmApsrPF1VO88EXnTO7XHOLcf7ifWBESlUvqMAEJ2ygNUhjwv85+KameUC/YDpQCvn3DrwQgLQMrjKwuJe4JdAechz8d7mDkAR8B//1MdjZtaAOG63c24NcDewClgHbHPOfUwct7mCqtqZkNu4aKMAEJ2skufi+vuaZtYQeA24yTm3Peh6wsnMTgMKnXOzg64lwlKA/sC/nXP9gJ3Ex6HvKvnnvM8E2gOZQAMzuzTYqqJCwm3jopECQHQqANqFPG6Ld9gwLplZKt7O/znn3Ov+0xvMrI0/vA1QGFR9YTAUOMPMVuCd3jnOzJ4lvtsM3nJd4Jyb7j9+FS8QxHO7TwCWO+eKnHP7gNeBo4jvNoeqqp0JtY2LVgoA0Wkm0NnM2ptZHbyLZd4OuKawMDPDOye80Dl3T8igt4HL/b8vB96KdG3h4pz7jXOurXMuF2/efuacu5Q4bjOAc249sNrMuvpPHQ98Q3y3exUw2Mzq+8v68XjXucRzm0NV1c63gQvNrK6ZtQc6AzMCqC+h6U6AUcrMTsE7T5wMPOGcuyPgksLCzIYBk4Cv+f58+P/gXQfwMpCNtxE9zzlX8QKjmGdmI4BbnXOnmVkL4rzNZtYX78LHOkA+cCXeB5G4bbeZ/QG4AO8bL18CY4CGxFmbzewFYATez/5uAP4PeJMq2mlmvwV+gtcvNznnPgig7ISmACAiIpKAdApAREQkASkAiIiIJCAFABERkQSkACAiIpKAFABEREQSkAKAiIhIAlIAEBERSUD/D4pOmdO7OzdLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Typical starting softmax cross-entropy loss (p = 0.1): ', -np.log(0.1))\n",
    "print('Decent ending softmax cross-entropy loss (p ~ 0.95 or so): ', -np.log(0.95))\n",
    "plt.plot(-np.log(np.linspace(0.1,0.99,100)))\n",
    "plt.title('Softmax cross-entropy loss starting at chance, ending at confident 99% prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the loss is sensitive to two things with the _relative values_ outputted by the network: maximizing the correctly classed example and minimizing the others.\n",
    "Therefore, it is possible for accuracy and loss to not track each other perfectly. Accuracy\n",
    "can stop improving while loss continues to decrease, typically meaning the network is becoming more confident than we want.\n",
    "\n",
    "Another thing to be aware of is these aren't really confidences, and it would be trivial to re-map these relative values to a more meaningful (e.g. more useful) set of confidences in that they more match experience with real world data. \n",
    "Its a sort of illusion to consider these as confidences, they only relate to relative scaling. The amount of confidence scaling applied with softmax can be manipulated with the \"temperature\" parameter. See online for examples.\n",
    "Also note with softmax cross-entropy, due to the softmax, the loss can continue to decrease while accuracy remains constant because the top-returned class can continue being correct at the same rate while the network learns to become less equivocal overly-certain in its output. A reduction in this kind of loss without an increase in accuracy may mean the network is becoming more brittle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression loss, L2 and L1\n",
    "l2_loss = lambda labels,output: -0.5*np.nanmean(np.power(output - labels, 2.0))\n",
    "l1_loss = lambda labels,output: -np.nanmean(np.abs(output - labels))\n",
    "# because we nearly always consider loss as a scalar, typically we take the mean above\n",
    "# however, gradient functions need to remain un-summed\n",
    "l2_loss_delta = lambda labels,output: -(output - labels)\n",
    "l1_loss_delta = lambda labels,output: -np.where(output>labels, \\\n",
    "                                         np.ones((len(labels),)), -np.ones((len(labels),)))\n",
    "# random data with and without outliers\n",
    "y = np.arange(100)\n",
    "loss_data = 5*np.random.random((100,))+y\n",
    "loss_data_outliers = np.copy(loss_data)\n",
    "inds=[4,8,15]\n",
    "loss_data_outliers[inds] = 100\n",
    "print('L1 Losses on clean, corrrupted data: ',[l1_loss(loss_data, y), \\\n",
    "                                               l1_loss(loss_data_outliers, y)])\n",
    "print('L2 Losses on clean, corrrupted data: ',[l2_loss(loss_data, y), \\\n",
    "                                               l2_loss(loss_data_outliers, y)])\n",
    "y = y.astype(np.float)\n",
    "# cross-entropy loss formulated for use in regression\n",
    "ce_regression_loss = lambda labels, output: -np.nansum(labels * \\\n",
    "                      np.log(sigmoid(output)) + (1 - labels) * np.log(1 - sigmoid(output)))\n",
    "print('CE Losses on clean, corrrupted data: ' \\\n",
    "      ,[ce_regression_loss(loss_data/100.0, y/100.0), \\\n",
    "        ce_regression_loss(loss_data_outliers/100.0, y/100.0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation for FC neurons\n",
    "\n",
    "To recap, we're working on a network with two layers: $ w1 = [512, 784], b1 = [512,1], w2 = [10,512], b2 = [10,1] $\n",
    "\n",
    "First (hidden) layer feature map: $$ z^1_j = (w^1_{ij} . x_i) + b^1_j $$\n",
    "\n",
    "activation of hidden layer map: $$ a^1_j = g(z^1_j) $$\n",
    "\n",
    "Second (output) layer map: $$ z^2_k = (w^2_{jk} . a^1_j) + b^2_k $$\n",
    "\n",
    "Final output after softmax: $$ S_k = \\frac{exp(z^2_k)}{sum_k(exp(z^2_k))}\n",
    "                    = \\frac{exp([w^2_{jk} * g((w^1_{ij} * x_i) + b_j)] +b^2_k)}{sum_k(exp([w^2_{jk} * g((w_{ij} * x_i) + b_j)] +b^2_k))} $$\n",
    "\n",
    "Loss is $ -log(S_i) $ and is a scalar, where $i==label$\n",
    "\n",
    "At each layer we'll be computing and back-propagating $\\frac{dL}{dz_{in}}$, where L=loss, $z_{in}$ is the feature map from the previous layer into the layer of interest. Then we'll be using the layer gradient to compute the weights gradient for the weights parameters, as $\\frac{dL}{dw_{i}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking down into components\n",
    "\n",
    "The gradient of the softmax loss w.r.t. activation coming from penultimate layer\n",
    "  $$ \\frac{dL}{dz} = \\frac{dL}{dS} * \\frac{dS}{da} * \\frac{da}{dz} $$\n",
    "\n",
    "Gradient w.r.t. weight (S=softmaxed output, a=feature output at layer i, W=weight at layer i):\n",
    "  $$ \\frac{dL}{dw_i} = \\frac{dL}{dS} * \\frac{dS}{da} * \\frac{da}{dz} * \\frac{dz}{dw}$$\n",
    "\n",
    "Gradient w.r.t. bias (b_i is bias at layer i):\n",
    "  $$ \\frac{dL}{db_i} = \\frac{dL}{dS} * \\frac{dS}{da} * \\frac{da}{dz} * \\frac{dz}{db} $$\n",
    "\n",
    "Breaking down the dz for weights and biases gives us:\n",
    "  $$\\frac{dz}{dw} = \\frac{d(w * activation_{i-1} + bias}{dw} = activation_{i-1} $$\n",
    "\n",
    "  $$\\frac{dz}{db} = \\frac{d(w * activation_{i-1} + bias)}{dbias} = 1 $$\n",
    "\n",
    "Breaking down the dL for softmax gives:\n",
    "$$\\frac{dL}{dS} = \\frac{d(-log(S_i))}{dS} = -\\frac{1}{S_i} $$\n",
    "\n",
    "## Softmax derivative\n",
    "The softmax derivative has two cases: $y==i$ and $y!=i$\n",
    "\n",
    "$$ \\frac{dS}{da} = S_k - S_k*S_i (k==i) and -S_k*S_i (k!=i)$$\n",
    "$$ \\frac{dS}{da} = S_i - S_i*S_i (k==i) and -S_k*S_i (k!=i)$$\n",
    "$$ \\frac{dS}{da} = S_i*onehot(label) - S_k*S_i$$\n",
    "\n",
    "where $S_k$ is the full softmaxed output vector and $S_i$ is (a scalar) made up of $S_k$ indexed at the label index. We can combine $\\frac{dS}{da}$ and $\\frac{dL}{dS}$ as softmax - onehot(label).\n",
    "\n",
    "## Weight decay\n",
    "We will also have an additional term in loss for weight decay (for every weight parameter): $$ L_{reg} = wd*0.5*sum(w^2) $$\n",
    "\n",
    "This second term will be added for each layer's weights gradient term: $$ \\frac{dL_{reg}}{dw} = wd*w $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_softmax_delta = lambda labels,output: output - one_hot(labels, output.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, we can write dL/dw at the second layer more simply:\n",
    "\n",
    "$$\\frac{dL}{dw} = - \\frac{1}{S_i} * (S_i*one\\_hot(label) - S_k*S_i) * (input\\_activation) + wd*w $$\n",
    "$$ = (S_k - one\\_hot(label)) * (input\\_activation) + wd*w$$\n",
    "\n",
    "Next, back-propagate the second layer's gradient to the first layer. Once we have that, we will have everything we need to implement backpropagation for this network.\n",
    "\n",
    "If we have the gradient at the second layer, we can simply reverse the forward operation and use the second layer's weights (transposed) to back-compute the gradient at the first layer. Note, this isn't quite the first layer gradient, because there was an activation layer between them, so we also have to partial in the activation gradient.\n",
    "\n",
    "$$dlayer_i = w^T_{i+1} * dlayer_{i+1} * \\frac{da_i}{dz_i}$$\n",
    "\n",
    "## Compute gradients\n",
    "Finally, we can run a forward pass and get the activations and features for both layers to prepare for gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations,features = forward(data, params, forward_only=False)\n",
    "softmax_outputs = softmax(activations[-1])\n",
    "loss = ce_loss(labels, activations[-1])\n",
    "print('Single forward pass loss on initialized network: ',loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the gradient for the last layer and the parameters associated with it. Notice we don't apply an activation derivative, because the only function applied to the last layer (softmax) has already been accounted for in the ce_softmax_delta() function. Also notice when computing the weights gradient from the layer gradient, the activation that flowed into this layer must be transposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_last_layer = ce_softmax_delta(labels, softmax_outputs)\n",
    "\n",
    "# assign bias grad\n",
    "grad_last_biases = grad_last_layer\n",
    "grad_last_weights = np.dot(grad_last_layer, activations[-2].transpose())\n",
    "# check shape, must match the weights shape\n",
    "assert grad_last_weights.shape == params[-1].weights.shape, 'gradient and weights shapes do not match'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the weight-decay term and sum into the weights gradient. Use an appropriately small weight-decay hyperparameter (0.0005 is common)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd=0.0005\n",
    "l2_last_weights = wd * params[-1].weights\n",
    "grad_last_weights += l2_last_weights\n",
    "print('Backprop, grad_last_layer shape: ',grad_last_layer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error propagation/backpropagate to the gradient for the next layer back.\n",
    " $$\\frac{dz_{out}}{dz_{in}} = \\frac{dz_{out}}{da_{in}} * \\frac{da_{in}}{dz_{in}} $$ \n",
    "\n",
    "Or, np.dot(next layer's weights, grad_last_layer above) * $\\frac{da_{in}}{dz_{in}}$.\n",
    "\n",
    "The gradient at a layer back is equal to the gradient at the next layer, backwardly multiplied by the next layer's weights times any partial derivative for anything happening between this layer and the next (e.g. any derivative due to the activation layer and as in the above case, the current layer's input activation). Recall the dot product used forward direction takes the input channels and transforms by inner product sum to get the output channels, so taking the dot of the transpose takes the gradient in the form of the layer's output channels and gets us back to the input channel dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_first_layer = np.dot(params[-1].weights.transpose(), grad_last_layer) * relu_delta(features[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus to keep pushing back further, to get the gradient at the previous layer, we first multiply the transpose of the current layer's weights by the current layer's gradient to get the gradient at the activation layer between the previous and current layers. Then multiply by the activation layer gradient to get the gradient at said layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_first_biases = grad_first_layer\n",
    "grad_first_weights = np.dot(grad_first_layer, activations[-3].transpose())\n",
    "assert grad_first_weights.shape == params[0].weights.shape, 'gradient and weights shapes do not match'\n",
    "\n",
    "# again get the weight-decay term (assuming we're using weight decay)\n",
    "l2_first_weights = wd * params[0].weights\n",
    "grad_first_weights += l2_first_weights\n",
    "print('Backprop, grad_first_layer shape: ',grad_first_layer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A few takeaways\n",
    "Small activations result in small gradients, and small weights in the next layer will result in small backprop gradients. \n",
    "\n",
    "Large activations can blow up if nothing is done to prevent them (e.g. weights regularization, gradient clamping).\n",
    "\n",
    "When using a clamping activation like relu, if something isn't activated the gradients will also be clamped. \n",
    "\n",
    "With most of the other nonlinear activations (e.g. sigmoid, tanh) that squish output nonlinearly, the derivative is very small for either large or small feature values, a condition commonly described as a \"saturated\" neuron. ReLU solves one end of this, leaky relu or prelu or selu were motivated to solve this issue.\n",
    "\n",
    "  grad_at_some_layer = np.recursive_dot( (next_layer_weights.T, next_layer_grads) * delta_activation(previous_layer_forward_output) )\n",
    "\n",
    "  grad_at_some_layer_weights = np.dot(grad_at_some_layer, activations_into_this_layer.T)\n",
    "\n",
    "This means that the backpropagated gradients grow geometrically in dependence on the subsequent weights and changes in activation as you move back. \n",
    "\n",
    "Next look at the gradient at the output we started with, cross-entropy vs L2\n",
    "\n",
    "  ce_grad_last_layer = (softmax(outputs) - one_hot(labels, outputs.shape[0]))\n",
    "\n",
    "  l2_grad_last_layer = (outputs - labels) * sigmoid_delta()\n",
    "\n",
    "Unfortunately, the sigmoid_delta can become 0 or 1 easily, and at that point, sigmoid_delta(close to 0 or close to 1) = 0, and the output gradient we want to start with becomes zero easily. Softmax cross-entropy does not have this issue. We can avoid this by avoiding a sigmoid on the output, but this results in an unconstrained output (can be handled in other ways)\n",
    "\n",
    "# Backpropagation functionalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_hybrid_onesample(data, label, params, loss_fn, loss_fn_delta, activation, activation_delta, wd=0.0005):\n",
    "    # activations (input to the next layer, equal to feature when no activation used)\n",
    "    a_l = [data]\n",
    "    # feature maps\n",
    "    f_l = []\n",
    "    delta_a_l = []\n",
    "    # compute the forward calculations and store the features and activations and the derivative of the parameters at these values\n",
    "    for layer in params:\n",
    "        # append feature and activation outputs\n",
    "        f_l.append(np.dot(layer.weights, a_l[-1]) + layer.bias)\n",
    "        # calc activation if specified, else just the raw feature map\n",
    "        a_l.append(layer.activation(f_l[-1]) if layer.activation is not None else f_l[-1])\n",
    "        # same for gradient, else just ones\n",
    "        delta_a_l.append(activation_delta(f_l[-1]) if layer.activation is not None else f_l[-1]/f_l[-1])\n",
    "\n",
    "    loss = loss_fn(label, a_l[-1])\n",
    "    # gradient at the loss\n",
    "    layerback = 1\n",
    "    # gradient at the last layer is the cost gradient times the derivative of the last activation layer\n",
    "    # in our toy network, we did no activation on last layer...\n",
    "    gradient_layer = loss_fn_delta(label, a_l[-1]) * delta_a_l[-1]\n",
    "    # gradients (in reverse order)\n",
    "    grad_w_reverse=[]\n",
    "    grad_b_reverse=[]\n",
    "    # store the gradients, bias is simply the gradient we compute above at this layer\n",
    "    grad_b_reverse.append(gradient_layer)\n",
    "    # weights gradient is this layer's gradient dotted with the previous layer's activation output (e.g. what this layer saw as input)\n",
    "    grad_w_reverse.append(np.dot(gradient_layer, a_l[-2].transpose()) + wd * params[-1].weights)\n",
    "\n",
    "    # backward further, subsequent layers depend on the following layer's activation_delta\n",
    "    N_layers = len(params)\n",
    "    for layerback in range(1, N_layers):\n",
    "        current_layer=N_layers - layerback\n",
    "        previous_layer=current_layer - 1\n",
    "        gradient_layer = np.dot(params[current_layer].weights.transpose(), gradient_layer) * delta_a_l[previous_layer]\n",
    "        # backpropagation: gradient at a given layer is equal to the next layer's weights dotted with the next layer's gradient\n",
    "        # then the bias' gradient is said gradient, and weights' gradient is said gradient dotted with the activations that were inputted to this layer\n",
    "        grad_b_reverse.append(gradient_layer)\n",
    "        grad_w_reverse.append(np.dot(gradient_layer, a_l[previous_layer].transpose()) + wd * params[previous_layer].weights)\n",
    "    return (grad_w_reverse, grad_b_reverse, loss, a_l[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get params for a one-hidden layer network with 64 units\n",
    "activation_fn=relu\n",
    "delta_activation_fn=relu_delta\n",
    "params = get_params(n_hidden=[64], activations=[activation_fn, None])\n",
    "# grab gradients for a single made-up example\n",
    "gradw_r,gradb_r,loss,output = forward_hybrid_onesample(data, labels, params, ce_loss, ce_softmax_delta, activation_fn, delta_activation_fn)\n",
    "print('Single pass thru initialized network, loss = ',loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About what we'd expect, the loss when all outputs are at chance should be log(0.1).\n",
    "\n",
    "# Check gradients numerically\n",
    "Modern platforms include things like auto-differentiation and numeric checks in their CI/automated tests, so you may only need to do this if you make new layers or check a new loss function for stability. The result should be on the order of the epsilon used (0.0001 here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "eps=0.0001\n",
    "dgwp=[]\n",
    "dgbp=[]\n",
    "for i in range(784):\n",
    "    datap=copy.deepcopy(data)\n",
    "    datan=copy.deepcopy(data)\n",
    "    datap[i] += eps\n",
    "    datan[i] -= eps\n",
    "    gwp,gbp,lp,op =  forward_hybrid_onesample(datap, labels, params, ce_loss, ce_softmax_delta, relu, relu_delta)\n",
    "    gwn,gbn,ln,on =  forward_hybrid_onesample(datan, labels, params, ce_loss, ce_softmax_delta, relu, relu_delta)\n",
    "    dgwp.append(sum([np.nansum(p-n) for p,n in zip(gwp,gwn)])/(2*eps*sum([np.prod(p.shape) for p in gwp])))\n",
    "    dgbp.append(sum([np.nansum(p-n) for p,n in zip(gbp,gbn)])/(2*eps*sum([np.prod(p.shape) for p in gbp])))\n",
    "dgwp=np.asarray(dgwp)\n",
    "dgbp=np.asarray(dgbp)\n",
    "print([np.prod(p.shape) for p in gbp])\n",
    "print([np.prod(p.shape) for p in gwp])\n",
    "plt.plot(dgbp)\n",
    "plt.title('bias gradient numeric check')\n",
    "plt.show()\n",
    "plt.plot(dgwp)\n",
    "plt.title('weights gradient numeric check')\n",
    "plt.show()\n",
    "print('min, max, mean weights delta',[np.min(dgwp), np.max(dgwp), np.nanstd(dgwp)])\n",
    "print('min, max, mean biases delta',[np.min(dgbp), np.max(dgbp), np.nanstd(dgbp)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader for MNIST\n",
    "Dead-simple, no bells and whistles beyond shuffling (randomizing the order of data samples). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import struct\n",
    "datasets=['data/train-labels-idx1-ubyte.gz', 'data/train-images-idx3-ubyte.gz']\n",
    "with gzip.open(datasets[0], 'rb') as fp:\n",
    "    # labels\n",
    "    magic=struct.unpack(\">L\", fp.read(size=4))[0]\n",
    "    assert magic == 2049, 'magic number in header does not match expectations'\n",
    "    num_items=struct.unpack(\">L\", fp.read(size=4))[0]\n",
    "    mnist_labels=np.asarray(list(fp.read()))\n",
    "\n",
    "with gzip.open(datasets[1], 'rb') as fp:\n",
    "    magic=struct.unpack(\">L\", fp.read(size=4))[0]\n",
    "    assert magic == 2051, 'magic number in header does not match expectations'\n",
    "    num_imgs=struct.unpack(\">L\", fp.read(size=4))[0]\n",
    "    num_rows=struct.unpack(\">L\", fp.read(size=4))[0]\n",
    "    num_cols=struct.unpack(\">L\", fp.read(size=4))[0]\n",
    "    data=np.asarray(list(fp.read()))\n",
    "    mnist_data=data.reshape((num_imgs, 28, 28, 1))\n",
    "    \n",
    "# randomize index and get data loader\n",
    "index=np.arange(num_items)\n",
    "np.random.shuffle(index)\n",
    "data_loader = zip([d for d in mnist_data[index,:,:,:]], [l for l in mnist_labels[index]])\n",
    "# not really a normalization just a rescaling to 0-1\n",
    "normalize_image = lambda img: (img-np.nanmin(img))/(np.nanmax(img)-np.nanmin(img)) if np.nanmax(img) > 0.0 else img\n",
    "print('created one-time use simple data_loader with n_images = ', num_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "Make a training loop and update the parameters with stochastic gradient descent. Display the first 5 data examples as images and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "batch_index=0\n",
    "wgrads_mov_avg=None\n",
    "# SGD with momentum\n",
    "update_mov_avg = lambda grads, batch_size, grads_mov_avg, momentum: [momentum * m + (1-momentum)*g/batch_size for g,m in zip(grads, grads_mov_avg)]\n",
    "loss_mov_avg = 0.0\n",
    "lr=0.1\n",
    "acc=[]\n",
    "grad_w_r_acc=None\n",
    "params_copy = copy.deepcopy(params)\n",
    "momentum=0.9\n",
    "for batchnum,batch in enumerate(data_loader):\n",
    "    img,lbl= batch\n",
    "    img=img.reshape((784,1)).astype(np.float32)\n",
    "    nimg=normalize_image(img)\n",
    "    gradw_r,gradb_r,loss,output = forward_hybrid_onesample(nimg, lbl, params_copy, ce_loss, ce_softmax_delta, activation_fn, delta_activation_fn)\n",
    "    loss_mov_avg += loss\n",
    "    maxind = np.argmax(softmax(output), axis=0)[0]\n",
    "    acc.append(maxind==lbl)\n",
    "    # initialize the batch gradient accumulator, will be re-set after each batch_size\n",
    "    if grad_w_r_acc is None:\n",
    "        gradw_r_acc = gradw_r\n",
    "        gradb_r_acc = gradb_r\n",
    "    else:\n",
    "        gradw_r_acc = [g1+g2 for g1,g2 in zip(gradw_r_acc, gradw_r)]\n",
    "        gradb_r_acc = [g1+g2 for g1,g2 in zip(gradb_r_acc, gradb_r)]\n",
    "    batch_index +=1\n",
    "    if batch_index==batch_size:\n",
    "        # accumulate a batch into the gradient tracker (SGD)\n",
    "        # recall the accumulator must be divided by batch_size as it is simply a sum over the batch\n",
    "        if wgrads_mov_avg is None:\n",
    "            wgrads_mov_avg = [w/batch_size for w in gradw_r_acc]\n",
    "            bgrads_mov_avg = [b/batch_size for b in gradb_r_acc]\n",
    "        else:\n",
    "            wgrads_mov_avg = update_mov_avg(gradw_r_acc, batch_size, wgrads_mov_avg, momentum)\n",
    "            bgrads_mov_avg = update_mov_avg(gradb_r_acc, batch_size, bgrads_mov_avg, momentum)\n",
    "        # Update parameters with SGD (learning_rate times the momentum-moving-averaged gradient)\n",
    "        for j in range(len(params_copy)):\n",
    "            params_copy[j].weights -= lr*wgrads_mov_avg[-j-1]\n",
    "            params_copy[j].bias    -= lr*bgrads_mov_avg[-j-1]\n",
    "        # reset the batch gradient accumulator\n",
    "        gradw_r_acc=None\n",
    "        batch_index = 0\n",
    "    if batchnum % (batch_size*100) == 1:\n",
    "        print('Batch %d, Loss avg: %.02f, acc: %.2f'%(int(batchnum/batch_size), loss_mov_avg/batchnum, sum(acc)/len(acc)))\n",
    "        if batchnum>batch_size*400:\n",
    "            break\n",
    "    # inspect the first few\n",
    "    if batchnum>=5:\n",
    "        continue\n",
    "    print('output: ', output)\n",
    "    plt.imshow(img.reshape((28,28)))\n",
    "    plt.title(lbl)\n",
    "    plt.show()\n",
    "print('Ran one Epoch')\n",
    "\n",
    "# save parameters to a dill file\n",
    "import dill\n",
    "with open('nnet_params_oneepoch.dill','wb') as fp:\n",
    "    listparams=[[p.weights,p.bias] for p in params_copy]\n",
    "    dill.dump(listparams, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train multiple epochs\n",
    "\n",
    "Perform N epochs of training, re-shuffling training data each time. You should see the loss gradually decrease and accuracy increase from a baseline of 0.1. Note if a moving average for some value (e.g. loss) gets reset on each epoch, you'll see it increase for the first batch, which you can ignore for now. How would you improve that?\n",
    "\n",
    "Many hyperparameters you can play with, from learning rate schedules, batch sizes, early stopping, weight decay and so forth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20\n",
    "reduction_epochs=[10, 15, 18]\n",
    "#activation_fn=sigmoid\n",
    "#delta_activation_fn=sigmoid_delta\n",
    "#params = get_params(n_hidden=[512, 256], activations=[activation_fn, activation_fn, None])\n",
    "params = get_params(n_hidden=[64], activations=[activation_fn, None])\n",
    "# accuracy accumulator\n",
    "acc_acc = []\n",
    "grad_w_r_acc=None\n",
    "wgrads_mov_avg=None\n",
    "for epoch in range(epochs):\n",
    "  if epoch in reduction_epochs:\n",
    "      lr=lr/1.5\n",
    "  batch_index=0\n",
    "  acc=[]\n",
    "  loss_mov_avg = 0.0\n",
    "  # randomize index and get data loader\n",
    "  index=np.arange(num_items)\n",
    "  np.random.shuffle(index)\n",
    "  data_loader = zip([d for d in mnist_data[index,:,:,:]], [l for l in mnist_labels[index]])\n",
    "  grad_w_r_acc=None\n",
    "  wgrads_mov_avg=None\n",
    "  # main training loop\n",
    "  for batchnum,batch in enumerate(data_loader):\n",
    "    img,lbl= batch\n",
    "    img=img.reshape((784,1)).astype(np.float32)\n",
    "    nimg=normalize_image(img)\n",
    "    randnoise=np.random.randn(784).reshape((784,1))*0.025\n",
    "    nimg=nimg+randnoise\n",
    "    # what would happen if we modified the image here before passing to forward, how would that affect the network's ability to learn?\n",
    "    # try adding some random noise...\n",
    "    gradw_r,gradb_r,loss,output = forward_hybrid_onesample(nimg, lbl, params, ce_loss, ce_softmax_delta, activation_fn, delta_activation_fn)\n",
    "    loss_mov_avg += loss\n",
    "    maxind = np.argmax(softmax(output), axis=0)[0]\n",
    "    maxind = np.argmax(softmax(output), axis=0)[0]\n",
    "    acc.append(maxind==lbl)\n",
    "    # initialize the gradient accumulators\n",
    "    if grad_w_r_acc is None:\n",
    "        gradw_r_acc = gradw_r\n",
    "        gradb_r_acc = gradb_r\n",
    "    else:\n",
    "        gradw_r_acc = [g1+g2 for g1,g2 in zip(gradw_r_acc, gradw_r)]\n",
    "        gradb_r_acc = [g1+g2 for g1,g2 in zip(gradb_r_acc, gradb_r)]\n",
    "    batch_index +=1\n",
    "    if batch_index==batch_size:\n",
    "        # initialize the moving-average gradient tracker for SGD\n",
    "        # recall, accumulator needs to be divided by batch_size so be careful using it blindly\n",
    "        if wgrads_mov_avg is None:\n",
    "            wgrads_mov_avg = [w/batch_size for w in gradw_r_acc]\n",
    "            bgrads_mov_avg = [b/batch_size for b in gradb_r_acc]\n",
    "        else:\n",
    "            wgrads_mov_avg = update_mov_avg(gradw_r_acc, batch_size, wgrads_mov_avg, momentum)\n",
    "            bgrads_mov_avg = update_mov_avg(gradb_r_acc, batch_size, bgrads_mov_avg, momentum)\n",
    "        for j in range(len(params)):\n",
    "            params[j].weights -= lr*wgrads_mov_avg[-j-1]\n",
    "            params[j].bias    -= lr*bgrads_mov_avg[-j-1]\n",
    "        gradw_r_acc=None\n",
    "        batch_index = 0\n",
    "    if batchnum % (batch_size*100) == 1:\n",
    "        print(' Batch %d, Loss avg: %.02f, acc: %.2f'%(int(batchnum/batch_size), loss_mov_avg/batchnum, sum(acc)/len(acc)))\n",
    "    # Note, we'll miss the last partial batch - can you think of a simple way to make use of these last images?\n",
    "    # many ways to do it, see if you can do it in as little as two lines moved above\n",
    "\n",
    "  acc_acc.append(sum(acc)/len(acc))\n",
    "  print('Epoch %d, Loss avg: %.02f, acc: %.2f'%(epoch, loss_mov_avg/batchnum, acc_acc[-1]))\n",
    "\n",
    "import dill\n",
    "with open('nnet_params.dill','wb') as fp:\n",
    "    listparams=[[p.weights,p.bias] for p in params]\n",
    "    dill.dump(listparams, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
