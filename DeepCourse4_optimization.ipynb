{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import signal\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Optimization with loss feedback\n",
    "\n",
    "This notebook will explore losses, particularly L1,L2 regression vs classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_loss = lambda labels,output: -0.5*np.nanmean(np.power(output - labels, 2.0))\n",
    "l1_loss = lambda labels,output: -np.nanmean(np.abs(output - labels))\n",
    "\n",
    "one_hot = lambda labels, n_classes: np.eye(n_classes)[np.array(labels).reshape(-1)].transpose()\n",
    "ce_loss = lambda labels,output: -np.nansum(np.log(softmax(output))* one_hot(labels, output.shape[0]), axis=0)\n",
    "softmax = lambda x: np.exp(x)/np.nansum(np.exp(x), axis=0)\n",
    "\n",
    "raw_outputs = np.random.randn(10)\n",
    "maxind = np.argmax(softmax(raw_outputs), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient at the loss has the magnitude of the label in it for all but L1 loss, which only has its polarity vs the output. L1 is like warmer/colder versus L2 is like a compass (but still not a map).\n",
    "\n",
    " L1 is robust to outliers but has major drawback of multiple solutions\n",
    " \n",
    " L1 just adds up the lengths in each of N dimensions which have many identical L1 norms \n",
    " \n",
    " L2 takes the vector length across the dimensions and can have only one solution\n",
    " \n",
    "# Data splits for training, validation and testing\n",
    "\n",
    "Data splitting means reserving some of the data for training and some for evaluating. Splitting is sometimes provided by the platform but necessary to understand. A typical split: 70:20:10 for train/validation/test. The validation is used to determine how to modify training hyperparameters and test is to provide a final unbiased (supposedly) evaluation of how well the network is going to perform on data not used to optimize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import struct\n",
    "datasets=['data/train-labels-idx1-ubyte.gz', 'data/train-images-idx3-ubyte.gz']\n",
    "with gzip.open(datasets[0], 'rb') as fp:\n",
    "    # labels\n",
    "    magic=struct.unpack(\">L\", fp.read(size=4))[0]\n",
    "    assert magic == 2049, 'magic number in header does not match expectations'\n",
    "    num_items=struct.unpack(\">L\", fp.read(size=4))[0]\n",
    "    mnist_labels=np.asarray(list(fp.read()))\n",
    "\n",
    "with gzip.open(datasets[1], 'rb') as fp:\n",
    "    magic=struct.unpack(\">L\", fp.read(size=4))[0]\n",
    "    assert magic == 2051, 'magic number in header does not match expectations'\n",
    "    num_imgs=struct.unpack(\">L\", fp.read(size=4))[0]\n",
    "    num_rows=struct.unpack(\">L\", fp.read(size=4))[0]\n",
    "    num_cols=struct.unpack(\">L\", fp.read(size=4))[0]\n",
    "    data=np.asarray(list(fp.read()))\n",
    "    mnist_data=data.reshape((num_imgs, 28, 28, 1))\n",
    "\n",
    "# split into train, val and held-back test sets\n",
    "index=np.arange(num_items)\n",
    "split1=int(num_items*0.7)\n",
    "split2=int(num_items*0.9)\n",
    "index_train=index[:split1]\n",
    "index_val=index[split1:split2]\n",
    "index_test=index[split2:]\n",
    "# randomize index (only for train set) and get data loader\n",
    "np.random.shuffle(index_train)\n",
    "train_data = zip([d for d in mnist_data[index_train,:,:,:]], [l for l in mnist_labels[index_train]])\n",
    "val_data = zip([d for d in mnist_data[index_val,:,:,:]], [l for l in mnist_labels[index_val]])\n",
    "test_data = zip([d for d in mnist_data[index_test,:,:,:]], [l for l in mnist_labels[index_test]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set should ideally ONLY be used to validate the finished network, to reduce opportunity for network to learn how to fake you out. The ideal scenario we're aiming at is a network that learns representations that generalize to new data. But we haven't seen the new data yet and it could be subtly different from the data we split into train/val/test. This is related to the \"unseen data\" problem.\n",
    "\n",
    "The effect of using test sets for directing training can be negligible or it can be catastrophic if you don't have additional realistic test sets to confirm. \n",
    "\n",
    "A much bigger effect however, is shift between 1) the train/val/test dataset distributions and 2) the real-world, production data distributions the network will encounter.\n",
    "\n",
    "# Class imbalance\n",
    "A related data sampling problem can be class imbalance. Say you have 90 examples of class 1 and 10 examples of class 2. Lets see what happens to an untrained 2-class network which should be 50% if unbiased by sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with an untrained, hand-crafted net:  0.9\n"
     ]
    }
   ],
   "source": [
    "n_images=2000\n",
    "labels=np.concatenate([np.ones((int(0.9*n_images),)), np.zeros((int(0.1*n_images),))])\n",
    "data=np.random.random((n_images,784,))\n",
    "trivial_inference = lambda data: np.ones(len(data),)\n",
    "trivial_accuracy = lambda labels, outputs: sum(labels==outputs)/float(n_images)\n",
    "outputs = trivial_inference(data)\n",
    "accuracy = trivial_accuracy(labels, outputs)\n",
    "print('Accuracy with an untrained, hand-crafted net: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There exist several techniques to handle imbalance, which fall approximately into three paths:\n",
    "\n",
    " 1. rebalance the sampling of what the net is exposed to\n",
    "\n",
    " 2. rebalance the gradients by downweighting according to classes exposed (in many cases this is about the same as rebalancing sampling)\n",
    "\n",
    " 3. modify the loss to be self-balancing or less sensitive to imbalance (e.g. focal loss, sampling-sensitive loss, generalized regression loss)\n",
    "\n",
    "Sampling considerations can be very subtle, for example, object detection requires learning from positive and negative examples. Typically, there will be far more potential negative examples (e.g. draw a box anywhere there isn't overlap with a label or use any label with the wrong class). Balancing negative/positive was necessary to get the first detectors working well.\n",
    "\n",
    "# Optimization with SGD vs Adam\n",
    "Stochastic gradient descent (SGD) is a shortcut to computing the total sample gradient.  Minibatches of data are passed thru network and gradients accumulated, then used to adjust using the moving average with momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_update_batch_grads = lambda grads, grads_mov_avg, momentum: [momentum * mov_grads + (1-momentum)*g for g,mov_grads in zip(grads, grads_mov_avg)]\n",
    "sgd_update_weights = lambda params, batch_grads, lr: [param - lr * delta for param,delta in zip(params, batch_grads)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive moment estimation is more sophisticated, it attempts to estimate the ideal gradient learning weight and momentum per parameter, described in detail in https://arxiv.org/pdf/1412.6980.pdf\n",
    "\n",
    "First, initialize gradient mean and variance as zero. Next, for each batch update with:\n",
    "\n",
    " $m = beta1 * m + (1 - beta1) * grad$\n",
    "\n",
    " $v = beta2 * v + (1 - beta2) * (grad^2)$\n",
    "\n",
    " $w = w - learning\\_rate * \\frac{m}{\\sqrt{v} + epsilon}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_update_movavg = lambda grads_m, grads_v, grads, beta1, beta2: [[beta1*m + (1-beta1)*g,beta2*v + (1-beta2)*g*g] for m,v,g in zip(grads_m, grads_v, grads)]\n",
    "adam_update_weights = lambda params, grads_m, grads_v, lr, time: [param - lr * delta for param,delta in zip(params, batch_grads)]\n",
    "learning_rate=0.001\n",
    "beta1=0.9\n",
    "beta2=0.999\n",
    "epsilon=1e-08\n",
    "time=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The moments (m and v) must be rescaled according to the number of times they've been accumulated\n",
    "\n",
    "$m = \\frac{m}{1 - beta1^{time}}$, where time = number of time steps\n",
    "\n",
    "$v = \\frac{v}{1 - beta2^{time}}$\n",
    "\n",
    "Thus m and v start out larger (by 1-beta) and trend towards the real m and v (10-fold larger for m and 500-fold larger in the case of v) this can be combined with learning rate to set the effective learning rate as a function of time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = learning_rate * np.sqrt(1-beta2**time)/(1-beta1**time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout and batchnorm\n",
    "\n",
    "Dropout simply takes a random mask at some dropout probability threshold and sets the activation values flowing in to zero.\n",
    "For backpropagation, it is essential to set the gradients to zero on each backward pass before we use them either to get the next layer back or when accumulating gradients, so the mask has to remain unchanged between forward and backward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = lambda data, mask: np.where(mask, data, np.zeros_like(data))\n",
    "dropout_backprop = lambda grads, mask: np.where(mask, grads, np.zeros_like(grads))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batchnorm builds a moving-average mean and standard deviation from data passed thru it. This is then used to normalize the data before doing anything else. The network is set up to learn a de-normalization. Specifically, it learns a gamma for the new mean and beta for new stddev.\n",
    "\n",
    "For backpropagation to learn the beta and gamma, its contributions are the derivative of the output (f_bn) with respect to gamma/beta:\n",
    "\n",
    " $\\frac{df_{bn}}{dbeta} = 1$\n",
    " \n",
    " $\\frac{df_{bn}}{dgamma} = data\\_norm$\n",
    "\n",
    "Where data_norm is the previous activation normalized by the moving average used to normalize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm(data, mov_avg_mu, mov_avg_sigma, gamma, beta, across_axis=0, eps=1e-5, momentum=0.9):\n",
    "    mean = np.mean(data)\n",
    "    var = np.sqrt(np.mean(np.mean(data - mean) ** 2))\n",
    "    mov_avg_mu = momentum*mov_avg_mu + (1-momentum)*mean\n",
    "    mov_avg_sigma = momentum*mov_avg_sigma + (1-momentum)*var\n",
    "    data_norm = (data - mov_avg_mu) / mov_avg_sigma\n",
    "    data_bn = (data_norm * gamma) + beta\n",
    "    return (data_bn,mov_avg_mu,mov_avg_sigma,gamma,beta)\n",
    "\n",
    "# batchnorm_delta returns data_norm to be used in a backward pass to multiply into the backward-working gradients\n",
    "# for gamma, for delta treat it like we treated the bias in fc layer backprop\n",
    "batchnorm_delta = lambda data, mov_avg_mu, mov_avg_sigma: (data - mov_avg_mu) / mov_avg_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
