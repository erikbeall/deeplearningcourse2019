
Code complementing Deep Learning 2019 Summer Course at Digilabs
Purpose of course is to develop understanding of neural networks as used in machine learning.
Part 1 will start before the lectures, the rest will follow in between each set of lectures until Part 7
all of which will follow the course.

Part 1: Preparation - Practical python for neural networks
Quick background on using python
a. Data/image reading/writing in python
b. Display image, histogram, image stats
c. Numpy basics, np.dot, matmul, conv2d
d. List (and other set) comprehension
e. Lambdas for simple purposes
f. Image processing fun example with 3 regressors (mean, trend and on/off block paradigm showing covariates)

Part 2: A simple neural network
What is a neural network, and how is it a universal approximator?
a. A fully connected neuron and its parameters
b. Activation functions
c. A simple neural network for digit classification
d. Regression vs classification
e. Load tuned weights and run a forward pass

Part 3: Finding the parameters
If we can't brute force search, how do we find parameters and what does that look like?
a. Abstractions for parameters and layers
b. Loss and backpropagation
c. The gradient as local approx to a brute-force calculation of each local derivative
d. Backpropagation in full
e. Simple data loader for digits
f. Training a simple network in numpy

Part 4: Optimization with loss feedback
A million ways to train a network, not all equal
a. Loss with L1,L2 regression vs classification
b. Data splits for training, validation and testing
c. Optimization with SGD and Adam
d. Dropout and batchnorm

Part 5: Convolutional neural networks
Receptive fields based on brains can do so much more, how do they work?
a. Convolutions
b. Convolutional layers and variants
c. Backpropagation with convolutions
d. Common architectures and their unique features
e. Deep dream in CNNs

Part 6: Recurrent neural networks
Memory makes it so much more
a. Gated recurrent unit and LSTM
b. How to deal with recurrent gradients
c. Turing completeness vs universal function approximators

Part 7: Practical with a platform: MXNet vs PyTorch vs Tensorflow
What does a typical data science with AI job look like and what are my tools
a. Simple network and training in each (Recommendation: Get comfortable with one! And then a second platform.)
b. Data augmentation
c. More hyperparameters
d. Examining statistics of the network parameters and feature maps
e. Tracking your training in more detail
f. Penultimate and prior layers have useful information (perceptual loss, clustering in a metric space)
g. Data loaders agnostic to the toolchain using it (user specifies normalization wanted and framework, provides self-tested dataloader)

Part 8: Abstractions for useful management of neural network development
At some point you will find yourself calculating how long an idea would take, here's what that might look like
If you don't, you might not be doing hard development (continuum from hard research, just a difference of degree)
a. Research as an open-ended pursuit of the unknowable
b. Research: tracking variants of research code and hyperparameters
c. Research: reporting results with statistical methods
d. Production as the management of powerful but imperfect perceptual machines
e. Production: Provenance vs "it worked well on a few validation and test sets"
f. Production: Tracking performance and reacting appropriately
g. Development as a combination of customization, extension and research
h. Practical, at some point you will find yourself calculating how long an idea would take, here's what that might look like

Part 9: Related compression methods of interest
Compressibility, dimensionality, sparse matrices and random projections, oh my!
a. PCA/ICA
b. Compressed sensing
c. Graph theory
d. Basis sets, how real analysis comes into learnability and its relevance for us in the real (constructable) world
e. Configuration space in condensed matter physics

Part 10: Project topics
I'm done, provide your own pithy summary!
a. Survey of interesting ideas and discussions (code for various projects)


